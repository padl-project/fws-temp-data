---
title: "Download Raw Data from Drive"
author: "Camila Vargas"
output: html_document
---

This script downloads data from a specific folder on our Google Drive

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(googledrive)
library(here)
library(tidyverse)
library(readxl)
library(janitor)


## Sets R not to use scientific notations
options(scipen=999) 

# If it does not exist, create raw data folders 

dir.create(here::here("6.raw_data"))

dir.create(here::here("7.clean_data"))


```

## Download data files into local computer

Make sure the file path on the code is updated to the correct folder.


```{r download weather files}
# url of folder where the data lives
# Copy paste the url of the folder where the data lives

## Allie Hunter_FWS
folder_url <- "https://drive.google.com/drive/folders/1MxBrDKefrRByxFhRKbvbzHnSDshcgW1e"

# list of files/folders inside the folder: Allie Hunter_FWS
files <- drive_ls(as_id(folder_url))

```

In this case there are too many sub-folders and we do not need all the files in the folders. Therefore I'll just go checking one by one and downloading the most raw version of the data.

```{r}
## files and folders inside SeaTempLogger2017

dir_17 <-  drive_ls(files[4, ])  ## modify number according to list in object files
 
## TESTING
 
## Download xlsx "Temperature Loggers Retrieved Aug 2017.xlsx" file to see what it has
drive_download(as_id(dir_17$id[1]),
               path = paste0("6.raw_data/", dir_17$name[1]),
               overwrite = T)

excel_sheets(paste0("6.raw_data/", dir_17$name[1]))

## Read excel file to see whet is has: These sheets are metadata!
meta_17 <- read_excel(paste0("6.raw_data/", dir_17$name[17]), sheet = "2017")
meta_18 <- read_excel(paste0("6.raw_data/", dir_17$name[17]), sheet = "2018")


## Sub-files inside SBE56-04744 Sacia Holei
subdir1 <- drive_ls(dir_17[3,])

## Download xlxs file in this forlder and check
drive_download(as_id(subdir1$id[1]),
               path = paste0("6.raw_data/", subdir1$name[1]),
               overwrite = T)

## Download csv file to compare data
drive_download(as_id(subdir1$id[2]),
               path = paste0("6.raw_data/", subdir1$name[2]),
               overwrite = T)

```

Checking if Excel file asn csv has the same information

```{r}
excel_sheets(paste0("6.raw_data/", subdir1$name[1]))

## Read excel to see data
xls_16 <- read_excel(paste0("6.raw_data/", subdir1$name[1]), sheet = "July2016_Dec2016")

xls_17 <- read_excel(paste0("6.raw_data/", subdir1$name[1]), sheet = "Jan2017_Aug2017")

xls_all <- bind_rows(xls_16, xls_17) %>% 
  select(1:4) %>%
  clean_names() %>% 
  mutate(date = as.Date(date, format = "YYYY-MM-DD"))

csv_8_17 <- read_csv(paste0("6.raw_data/", subdir1$name[2]), skip = 11)

setdiff(xls_all$sample_number, csv_8_17$`Sample Number`)
  
```


## Next steps for downloading all csv files
1. Build a data frame with all the files in the folders inside dir_17

```{r}
## Loop to name objects as they are being read. This loop creates an object for each sub-directory in dir17, listing each of the files in each folder.
## NOTE: Everytime a re run this scrito the order of the files changes. drive_ls() only works when looking into a folder, not a file. So, modify brakets numberes as needed!

for (i in 2:14){

  assign(paste0("subdir", i), drive_ls(dir_17[i,])) %>% 
    mutate(origin_dir = i)
}


## building one data frame with all files inside all forlders in dir_17

all_subdir <- bind_rows(subdir1, subdir2, subdir3, subdir4, subdir5, subdir6, subdir7, subdir8, subdir9, subdir10, subdir11, subdir12, subdir13, subdir14, .id = "source")

```


2.filter all the csv files and add site name

```{r}
csv_list <- all_subdir %>% 
  filter(str_detect(name, ".csv")) %>% 
  mutate(source = as.numeric(source))

site_names <- dir_17 %>% 
  select(name) %>% 
  separate(col = name,
           into = c("id1", "id2", "id3", "id4"),
           sep = " ",
           remove = FALSE) %>% 
  unite(site_name, id2, id3, id4, sep = " ", na.rm = T) %>% 
  mutate(source = 1:n()) %>% 
  filter(source %in% c(1:14)) %>% 
  select(source, folder_name = name, site_name)


csv_w_names <- csv_list %>% 
  left_join(site_names, by = "source")

```

3. download all csv files - add an identifier string in the file name to know the file comes form the 2017 folder


```{r}

## Download all file to local computer. 
## This code did not work. Error in curl::curl_fetch_disk(url, x$path, handle = handle) : Failed to open file raw_data/Sacia Holei_SBE05604744_2017-08-30.csv.

# purrr::walk2(
#     map(csv_w_names$id, as_id),
#     paste0("raw_data/",csv_w_names$site_name, "_", csv_w_names$name),
#     drive_download,
#     overwrite = TRUE)


for (i in seq(1:18)){

  drive_download(as_id(csv_list$id[i]),
               path = paste0("6.raw_data/", csv_w_names$site_name[i], "_", csv_w_names$name[i]),
               overwrite = T)
  
}

```

-----------------------------------------------------------------------------------------------
### Download files for Subfolder 2018

```{r}
## files and folders inside SeaTempLogger2018

dir_18 <-  drive_ls(files[1, ]) ## **check file to make sure 5 correspond to the 2018 folder**
  
```

1. Build a data frame with all the files in the folders inside dir_18

```{r}
## Loop to name objects as they are being read. Missing the part of binding all together into one dataframe

for (i in 2:10){

  assign(paste0("subdir", i), drive_ls(dir_18[i,])) %>% 
    mutate(origin_dir = i)
}


## Create a list with all subdir files
subdir_list <- list(subdir2, subdir3, subdir4, subdir5, subdir6, subdir7, subdir8, subdir9, subdir10, dir_18)

## Re name lists inside list with corresponding object name
names(subdir_list) <- c("subdir2", "subdir3", "subdir4", "subdir5", "subdir6", "subdir7", "subdir8", "subdir9", "subdir10", "dir_18")

## Bind all data frames into one
all_subdir <- bind_rows(subdir_list, .id = "source")

```


2.filter all the csv files and add site name

```{r}
csv_list <- all_subdir %>% 
  filter(str_detect(name, ".csv"))

site_names <- dir_18 %>% 
  select(name) %>% 
  mutate(site_name = str_replace(name, "_", " "),
         site_name = str_remove_all(site_name, "\\d+"),
         site_name = str_remove_all(site_name, "\\-"),
         site_name = str_remove_all(site_name, "\\_"),
         site_name = str_remove_all(site_name, ".csv") %>% trimws(.),
         source = paste0("subdir", 1:11),
         source = ifelse(source == "subdir1", "dir_18", source)) %>% 
  filter(source != "subdir11") %>% 
  select(source, folder_name = name, site_name)


csv_w_names <- csv_list %>% 
  left_join(site_names, by = "source")

```

3. download all csv files - add an identifier string in the file name to know the file comes form the 2018 folder


```{r}

## Download all file to local computer. 
## This code did not work. Error in curl::curl_fetch_disk(url, x$path, handle = handle) : Failed to open file raw_data/Sacia Holei_SBE05604744_2017-08-30.csv.

# purrr::walk2(
#     map(csv_w_names$id, as_id),
#     paste0("raw_data/",csv_w_names$site_name, "_", csv_w_names$name),
#     drive_download,
#     overwrite = TRUE)


for (i in seq(1:10)){

  drive_download(as_id(csv_list$id[i]),
               path = paste0("6.raw_data/", csv_w_names$site_name[i], "_", csv_w_names$name[i]),
               overwrite = T)
  
}

```


-------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------
### Download files for Subfolder 2020

```{r}
## files and folders inside SeaTempLogger2020

dir_20 <-  drive_ls(files[5, ]) ## **check file to make sure 5 correspond to the 2018 folder**
  
```

1. Build a data frame with all the files in the folders inside dir_18

```{r}
## Loop to name objects as they are being read. Missing the part of binding all together into one dataframe

for (i in 1:9){

  assign(paste0("subdir", i), drive_ls(dir_20[i,])) %>% 
    mutate(origin_dir = i)
}


## Create a list with all subdir files
subdir_list <- list(subdir1, subdir2, subdir3, subdir4, subdir5, subdir6, subdir7, subdir8, subdir9)

## Re name lists inside list with corresponding object name
names(subdir_list) <- c("subdir1", "subdir2", "subdir3", "subdir4", "subdir5", "subdir6", "subdir7", "subdir8", "subdir9")


## Bind all data frames into one
all_subdir <- bind_rows(subdir_list, .id = "source")

```


2.filter all the csv files and add site name

```{r}
csv_list <- all_subdir %>% 
  filter(str_detect(name, ".csv"))
## Note: only 8 files because folder for site Fighter Strip is empty, missing file!

site_names <- dir_20 %>% 
  select(name) %>% 
  slice_head(n = 9) %>% 
  mutate(site_name = str_replace(name, "_", " "),
         site_name = str_remove_all(site_name, "\\d+"),
         site_name = str_remove_all(site_name, "\\-"),
         site_name = str_remove_all(site_name, "\\_"),
         site_name = str_remove_all(site_name, ".csv") %>% trimws(.),
         serial_num = str_extract_all(name, "\\d+") %>% trimws(.),
         source = paste0("subdir", 1:9)) %>% 
  select(source, folder_name = name, site_name, serial_num)


csv_w_names <- csv_list %>% 
  left_join(site_names, by = "source")

```

3. download all csv files 


```{r}

## Download all file to local computer. 
## This code did not work. Error in curl::curl_fetch_disk(url, x$path, handle = handle) : Failed to open file raw_data/Sacia Holei_SBE05604744_2017-08-30.csv.

# purrr::walk2(
#     map(csv_w_names$id, as_id),
#     paste0("raw_data/",csv_w_names$site_name, "_", csv_w_names$name),
#     drive_download,
#     overwrite = TRUE)


for (i in seq(1:8)){

  drive_download(as_id(csv_list$id[i]),
               path = paste0("6.raw_data/", csv_w_names$site_name[i], "_", csv_w_names$name[i]),
               overwrite = T)
  
}

```


--------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------
### Download files for Subfolder 2021

```{r}
## files and folders inside SeaTempLogger2021

dir_21 <-  drive_ls(files[1, ]) ## **check file to make sure 5 correspond to the 2018 folder**
  
```

1. Build a data frame with all the files in the folders inside dir_18

```{r}
## Loop to name objects as they are being read. Missing the part of binding all together into one dataframe

for (i in 1:5){

  assign(paste0("subdir", i), drive_ls(dir_21[i,])) %>% 
    mutate(origin_dir = i)
}


## Create a list with all subdir files
subdir_list <- list(subdir1, subdir2, subdir3, subdir4, subdir5)

## Re name lists inside list with corresponding object name
names(subdir_list) <- c("subdir1", "subdir2", "subdir3", "subdir4", "subdir5")


## Bind all data frames into one
all_subdir <- bind_rows(subdir_list, .id = "source")

```


2.filter all the csv files and add site name

```{r}
csv_list <- all_subdir %>% 
  filter(str_detect(name, ".csv"))


site_names <- dir_21 %>% 
  select(name) %>% 
  mutate(site_name = str_replace(name, "_", " "),
         site_name = str_remove_all(site_name, "\\d+"),
         site_name = str_remove_all(site_name, "\\-"),
         site_name = str_remove_all(site_name, "\\_"),
         site_name = str_remove_all(site_name, ".csv") %>% trimws(.),
         serial_num = str_extract_all(name, "\\d+") %>% trimws(.),
         source = paste0("subdir", 1:5)) %>% 
  select(source, folder_name = name, site_name, serial_num)


csv_w_names <- csv_list %>% 
  left_join(site_names, by = "source")

```

3. download all csv files 


```{r}

## Download all file to local computer. 
## This code did not work. Error in curl::curl_fetch_disk(url, x$path, handle = handle) : Failed to open file raw_data/Sacia Holei_SBE05604744_2017-08-30.csv.

# purrr::walk2(
#     map(csv_w_names$id, as_id),
#     paste0("raw_data/",csv_w_names$site_name, "_", csv_w_names$name),
#     drive_download,
#     overwrite = TRUE)


for (i in seq(1:5)){

  drive_download(as_id(csv_list$id[i]),
               path = paste0("6.raw_data/", csv_w_names$site_name[i], "_", csv_w_names$name[i]),
               overwrite = T)
  
}

```






